{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e130cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition as fr\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import cv2, dlib, math\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b3cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base =  r\".\\dataset\"\n",
    "names = os.listdir(base)                # names of people\n",
    "person_face_encodings = dict()\n",
    "\n",
    "\n",
    "for name in names:\n",
    "    person_face_encodings[name] = []\n",
    "    for file_name in os.listdir(base+'/'+name):\n",
    "        img = fr.load_image_file(base+'/'+name+'/'+file_name)\n",
    "        encodings = fr.face_encodings(img, model='large')\n",
    "        if len(encodings):\n",
    "            person_face_encodings[name].append(encodings[0])\n",
    "\n",
    "try:\n",
    "    f = open(\"./important files/face_encodings.pickle\", 'wb')\n",
    "    pickle.dump(person_face_encodings, f)\n",
    "    f.close()\n",
    "except:\n",
    "    print(\"Something went wrong!\")\n",
    "    os._exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4300af",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    f = open(\"./important files/face_encodings.pickle\", 'rb')\n",
    "    person_face_encodings = pickle.load(f)\n",
    "    f.close()\n",
    "except:\n",
    "    print(\"Something went wrong!\")\n",
    "    os._exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d636f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = dict()\n",
    "\n",
    "a = 0\n",
    "for p in list(person_face_encodings.keys()):\n",
    "    target[p] = a\n",
    "    a += 1\n",
    "reverse_target = dict([(v,k) for k,v in target.items()])\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfffd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for k,v in person_face_encodings.items():\n",
    "    for i in v:\n",
    "        X.append(i)\n",
    "        y.append(target[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e577f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34246bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "\n",
    "knn.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb133d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7da7553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7e0a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detection_2(img):\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray_img)\n",
    "    cropped_faces = []\n",
    "    take = 7  # increase the size of the bounding box\n",
    "    for face in faces:\n",
    "        cropped_faces.append(img[ face.top()-take :face.bottom()+take, face.left()-take:face.right()+take , :].copy())\n",
    "    for face in faces:\n",
    "        cv2.rectangle(img,(face.left()-5, face.top()-5), (face.right()+5, face.bottom()+5) ,(0,255,0), 5)\n",
    "    return cropped_faces , img\n",
    "\n",
    "def recognize_faces(img_path):\n",
    "    try:\n",
    "        file = open(\"./face_encodings.pickle\", 'rb')\n",
    "        person_face_encodings = pickle.load(file)\n",
    "        file.close()\n",
    "    except:\n",
    "        print(\"Something went wrong!\")\n",
    "        os._exit(1)\n",
    "    img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "    img = np.uint8(img)\n",
    "    cropped_faces, img = face_detection_2(img)\n",
    "    # print(len(cropped_faces), cropped_faces[0].shape)\n",
    "    # plt.imshow(img)\n",
    "    # plt.show()\n",
    "\n",
    "    ans = dict()\n",
    "    face_vs_name = []\n",
    "    for i,face in enumerate(cropped_faces):\n",
    "        # plt.imshow(face)\n",
    "        # plt.show()\n",
    "        unknown_encoding = fr.face_encodings(face ,num_jitters=3 , model='large')\n",
    "        if len(unknown_encoding):\n",
    "            ans = knn.predict(unknown_encoding)\n",
    "            face_vs_name.append(reverse_target[ans[0]])\n",
    "        else :\n",
    "            face_vs_name.append(\"~ENCODING\")\n",
    "\n",
    "    fig, axes = plt.subplots( math.ceil(len(face_vs_name)/2) , 2 )\n",
    "    for i,face in enumerate(cropped_faces):\n",
    "        axes.ravel()[i].imshow(face)\n",
    "        axes.ravel()[i].set_title(face_vs_name[i])\n",
    "        axes.ravel()[i].axis('off')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return face_vs_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a0825",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognize_faces(r\"./test images/1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognize_faces(r\"./test images/6.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e19a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    f = open(\"./important files/facenet_knn.pickle\", 'wb')\n",
    "    pickle.dump(knn, f)\n",
    "    f.close()\n",
    "except:\n",
    "    print(\"Something went wrong!\")\n",
    "    os._exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed3f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    f = open(\"./important files/facenet_knn.pickle\", 'rb')\n",
    "    model = pickle.load(f)\n",
    "    f.close()\n",
    "except:\n",
    "    print(\"Something went wrong!\")\n",
    "    os._exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639d7c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83914b70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d74903f3c0a8ddc14f9ca93275717badad87b72706d588d6160414d0927f0c5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
